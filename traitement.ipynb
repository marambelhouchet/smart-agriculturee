{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4188 images belonging to 1 classes.\n",
      "Found 4188 images belonging to 1 classes.\n",
      "Found 272 images belonging to 4 classes.\n",
      "Found 67 images belonging to 4 classes.\n",
      "Found 1214 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "# Define paths for plant diseases dataset\n",
    "plant_diseases_dataset = \"C:\\\\Users\\\\user\\\\Desktop\\\\d\"\n",
    "\n",
    "# Define ImageDataGenerator for data augmentation and normalization\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255, \n",
    "    shear_range=0.2, \n",
    "    zoom_range=0.2,  \n",
    "    horizontal_flip=True)  \n",
    "\n",
    "# Generate batches of augmented data from the directory\n",
    "batch_size = 64\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    plant_diseases_dataset,\n",
    "    target_size=(224, 224),  \n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "# Create a generator for the test dataset\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    plant_diseases_dataset,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  \n",
    ")\n",
    "\n",
    "# Define paths for soil dataset\n",
    "soil_train_dir = 'C:\\\\Users\\\\user\\\\Desktop\\\\Dataset\\\\test'\n",
    "soil_test_dir = 'C:\\\\Users\\\\user\\\\Desktop\\\\Dataset\\\\Train'\n",
    "\n",
    "# Create ImageDataGenerator for training and testing soil data\n",
    "soil_train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "soil_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create generators for soil data\n",
    "soil_train_generator = soil_train_datagen.flow_from_directory(\n",
    "    soil_train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training')\n",
    "\n",
    "soil_validation_generator = soil_train_datagen.flow_from_directory(\n",
    "    soil_train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation')\n",
    "\n",
    "soil_test_generator = soil_test_datagen.flow_from_directory(\n",
    "    soil_test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical')\n",
    "\n",
    "# Define and compile the model\n",
    "def build_model(num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.001)  # Adam optimizer with learning rate\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Build and train the models\n",
    "plant_model = build_model(train_generator.num_classes)\n",
    "soil_model = build_model(soil_train_generator.num_classes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2s/step - accuracy: 0.2709 - loss: 2.8721 - val_accuracy: 0.6562 - val_loss: 0.9829\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.5938 - loss: 0.9906 - val_accuracy: 0.3333 - val_loss: 0.9756\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.6094 - loss: 0.9672 - val_accuracy: 0.6562 - val_loss: 0.7302\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.5938 - loss: 0.7896 - val_accuracy: 0.6667 - val_loss: 0.7419\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.6815 - loss: 0.7342 - val_accuracy: 0.6406 - val_loss: 0.7561\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7500 - loss: 0.6104 - val_accuracy: 0.0000e+00 - val_loss: 1.0732\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.7760 - loss: 0.5491 - val_accuracy: 0.8438 - val_loss: 0.4412\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.8438 - loss: 0.3704 - val_accuracy: 1.0000 - val_loss: 0.4008\n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - accuracy: 0.8384 - loss: 0.3600 - val_accuracy: 0.8125 - val_loss: 0.4341\n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7188 - loss: 0.7437 - val_accuracy: 1.0000 - val_loss: 0.2458\n"
     ]
    }
   ],
   "source": [
    "# Train the soil model\n",
    "soil_history = soil_model.fit(\n",
    "    soil_train_generator,\n",
    "    steps_per_epoch=soil_train_generator.samples // soil_train_generator.batch_size,\n",
    "    validation_data=soil_validation_generator,\n",
    "    validation_steps=soil_validation_generator.samples // soil_validation_generator.batch_size,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses\\losses.py:27: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return self.fn(y_true, y_pred, **self._fn_kwargs)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 4s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0000e+00 \n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0000e+00 \n",
      "Epoch 5/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 772us/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0000e+00 \n",
      "Epoch 9/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 3s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0000e+00 \n"
     ]
    }
   ],
   "source": [
    "history = plant_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 473ms/step - accuracy: 0.8597 - loss: 0.5654\n",
      "Soil model test accuracy: 0.8418451547622681\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Plant disease model test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "soil_test_loss, soil_test_acc = soil_model.evaluate(soil_test_generator)\n",
    "print(f'Soil model test accuracy: {soil_test_acc}')\n",
    "\n",
    "plant_test_loss, plant_test_acc = plant_model.evaluate(test_generator)\n",
    "print(f'Plant disease model test accuracy: {plant_test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "soil_model.save('soil_classification_model.h5')\n",
    "plant_model.save('plant_disease_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 732ms/step\n",
      "Predicted soil class: Black Soil\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
      "Predicted plant disease class: Blight\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the models\n",
    "soil_model = load_model('soil_classification_model.h5')\n",
    "plant_model = load_model('plant_disease_model.h5')\n",
    "\n",
    "# Define class indices for soil\n",
    "soil_class_indices = {\n",
    "    'Alluvial soil': 0,\n",
    "    'Black Soil': 1,\n",
    "    'Clay soil': 2,\n",
    "    'Red soil': 3\n",
    "}\n",
    "\n",
    "# Define class indices for plant diseases\n",
    "plant_class_indices = {\n",
    "    'Blight': 0,\n",
    "    'Common_Rust': 1,\n",
    "    'Gray_Leaf_Spot': 2,\n",
    "    'Healthy': 3\n",
    "}\n",
    "\n",
    "def predict_soil(image_path, class_indices):\n",
    "    image = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image) / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    predictions = soil_model.predict(image)\n",
    "    predicted_class = np.argmax(predictions, axis=1)\n",
    "    return class_indices, predicted_class[0]\n",
    "\n",
    "def predict_plant_disease(image_path, class_indices):\n",
    "    image = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image) / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    predictions = plant_model.predict(image)\n",
    "    predicted_class = np.argmax(predictions, axis=1)\n",
    "    return class_indices, predicted_class[0]\n",
    "\n",
    "# Test the models with sample images\n",
    "soil_image_path = 'C:\\\\Users\\\\user\\\\Desktop\\\\soil.jpg'  \n",
    "plant_image_path = 'C:\\\\Users\\\\user\\\\Desktop\\\\mm.jpg'  \n",
    "\n",
    "# Predict soil type\n",
    "soil_class_indices, predicted_soil_class = predict_soil(soil_image_path, soil_class_indices)\n",
    "print(f'Predicted soil class: {list(soil_class_indices.keys())[list(soil_class_indices.values()).index(predicted_soil_class)]}')\n",
    "\n",
    "# Predict plant disease\n",
    "plant_class_indices, predicted_plant_class = predict_plant_disease(plant_image_path, plant_class_indices)\n",
    "print(f'Predicted plant disease class: {list(plant_class_indices.keys())[list(plant_class_indices.values()).index(predicted_plant_class)]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
